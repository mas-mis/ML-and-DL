{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Code snippet for scraping books from rokomari","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport requests\nfrom csv import writer\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-10-14T19:38:13.963016Z","iopub.execute_input":"2021-10-14T19:38:13.963437Z","iopub.status.idle":"2021-10-14T19:38:14.198825Z","shell.execute_reply.started":"2021-10-14T19:38:13.963308Z","shell.execute_reply":"2021-10-14T19:38:14.198038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"home_url = \"https://www.rokomari.com\"\nbase_url = \"https://www.rokomari.com/book/category/1983/extra-discount?ref=act_pg0_p0&page=\"\n\ntitle = []\nauthors = []\nprices_original = []\nprices_selling = []\ndiscount = []\ncategory = []\npublishers = []\nratings_count = []\nratings = []\n\nchar_to_replace = {\n    '\\n': ''\n}\n\nfor i in range(50):\n    url = base_url + str(i+1)\n    page = requests.get(url)\n    soup = BeautifulSoup(page.content, \"lxml\")\n    lists = soup.find_all('div', class_=\"home-details-btn-wrapper\")\n    \n    \n    for list in lists:\n        for link in list.find_all('a', href=True):\n            url_details = home_url + link[\"href\"]\n            page_details = requests.get(url_details)\n            soup_details = BeautifulSoup(page_details.content, \"lxml\")\n            \n            lists_details = soup_details.find_all('div', class_=\"col details-book-main-info align-self-center\")\n            lists_authors = soup_details.find_all('p', class_=\"details-book-info__content-author\")\n            lists_cat = soup_details.find_all('div', class_=\"details-book-info__content-category d-flex align-items-center\")\n            lists_ratings_count = soup_details.find_all('div', 'span', class_=\"details-book-info__content-rating\")\n            lists_ratings = soup_details.find_all('div', class_=\"media ratings-review__content--rating\")\n            lists_price = soup_details.find_all('div', 'span', class_=\"details-book-info__content-book-price\")\n            \n            lists_pub = soup_details.find_all('td', class_=\"publisher-link\")    \n            \n# # # __________________________________Scraping the title of the books_____________________________\n            for list_det in lists_details:\n                for details in list_det.find_all('div', class_=\"details-book-main-info__header\"):\n                    title.append(details.find('h1').text)\n                    \n#___________________________________Scraping the name of the authors______________________________\n            for author in lists_authors:\n                authors.append(author.a.text.translate(str.maketrans(char_to_replace)).strip())\n\n# __________________________________Scraping the category______________________\n            for cat in lists_cat:\n                category.append(cat.a.text.strip())\n#___________________________________Scraping the ratings count________________________\n            for rating_count in lists_ratings_count:\n                rating_count_info = rating_count.span\n                if rating_count_info == None:\n                    ratings_count.append('NA')\n                else:\n                    ratings_count.append(rating_count_info.text)\n                    \n#___________________________________Scraping the ratings (this section has bug)________________________\n            for rating in lists_ratings:\n                rating_info = rating.find('h2', class_=\"pt-2\")\n                if rating_info == None:\n                    ratings.append('NA')\n                else:\n                    ratings.append(rating_info.text.replace('\\n', '').strip())\n\n# # ___________________________________scraping the prices_____________________\n\n            for price in lists_price:\n                prices_selling.append(price.find('span', class_=\"sell-price\").text.replace('\\n', ''))\n                price_origin_info = price.find('strike', class_=\"original-price\")\n                if price_origin_info == None:\n                    prices_original.append('NA')\n                    discount.append('Na')\n                else:\n                    prices_original.append(price_origin_info.text.replace('\\n', ''))\n                    discount.append(price.find('span', class_=\"js--save-message\").text.replace('\\n', ''))\n\n#____________________________________Scraping the publications_________________\n                                \n            for pub in lists_pub:\n                publishers.append(pub.a.string.replace('\\n','').strip())\n                \n        \n\n#****************************  Rearranging all the scraped data ******************************\n                \ndict_book = {\n    \"Title\": title,\n    \"Author\": authors,\n    \"Original Price\": prices_original,\n    \"Selling Price\": prices_selling,\n    \"Discount\": discount,\n    \"Publisher\": publishers,\n    \"Rating Count\": ratings_count,\n    \"Category\": category\n}\nbook_df = pd.DataFrame(dict_book)   \nbook_df.to_csv('More Info Rokomari.csv', index=False) # Exporting to a csv file","metadata":{"execution":{"iopub.status.busy":"2021-10-14T20:50:20.584801Z","iopub.execute_input":"2021-10-14T20:50:20.585087Z","iopub.status.idle":"2021-10-14T20:50:20.594975Z","shell.execute_reply.started":"2021-10-14T20:50:20.585045Z","shell.execute_reply":"2021-10-14T20:50:20.593552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trying To Get the URL","metadata":{}},{"cell_type":"code","source":"home_url = \"https://www.rokomari.com\"\nbase_path = \"https://www.rokomari.com/book/publishers?ref=sm_p2\"\n\npublisher = []\nurl = []\n\nfor i in range(3):\n    if i<1:\n        page = requests.get(base_path)\n    else:\n        page = requests.get(base_path+'&page='+str(i+1))\n    soup = BeautifulSoup(page.content, \"lxml\")\n    lists = soup.find_all('ul', class_=\"list-inline list-unstyled authorList\")\n    for list in lists:\n        for name in list.find_all('h2'):\n            publisher.append(name.text)\n        for link in list.find_all('a', href=True):\n            url.append(home_url + link[\"href\"]) \n\ninfo_dict = {\n    \"Publishers Name\": publisher,\n    \"URL\": url\n}\ninfo_df = pd.DataFrame(info_dict)\ninfo_df.to_csv('Publishers_url.csv', index=False)    ","metadata":{"execution":{"iopub.status.busy":"2021-10-12T01:45:18.164876Z","iopub.execute_input":"2021-10-12T01:45:18.16555Z","iopub.status.idle":"2021-10-12T01:47:13.659494Z","shell.execute_reply.started":"2021-10-12T01:45:18.165514Z","shell.execute_reply":"2021-10-12T01:47:13.658757Z"},"trusted":true},"execution_count":null,"outputs":[]}]}