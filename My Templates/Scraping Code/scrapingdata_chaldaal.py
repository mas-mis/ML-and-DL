# -*- coding: utf-8 -*-
"""ScrapingData_CHALDAAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ceMYL3TQXlsQiUSN2997z3VyfCinUADs
"""

# Let's start with importing some packages
import requests
from bs4 import BeautifulSoup
import pandas as pd

"""Let's set the base URL of the main page because we'll need that when we construct our URLs for each of the individual products.

Also, let's send a user-agent on every HTTP request, because if we make GET request using requests then by default the user-agent is Python which might get blocked.

So, to override that, we will declare a variable which will store our user-agent.
"""

baseurl = "https://evaly.com.bd/"
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}

"""Now, investigate the page so that we can figure out where the links are and how we're going to get them. You have to open Chrome dev tools by using inspect. We find that the products are listed under the div, with class = 'CategoryProductWithPagination__Grid-sc-ykaib3-0 ebgRSy my-4'. Let's use BeautifulSoup to get the list of items. Put it in a list (productlist). Now, generate an empty list and loop over the productlist to extratc the embabed link for each product. Print out the product link.

Note: we will send a user-agent on every HTTP request, because if you make GET request using requests then by default the user-agent is Python which might get blocked.

So, to override that, we will declare a variable which will store our user-agent.

Here, I am scripting data from Chaldal.com for their popular products only. You'll need to change according to the website you are aiming for.

Let's have a quick look at the product links.
"""

baseurl3 = "https://chaldal.com"
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}

r = requests.get('https://chaldal.com/popular')
soup=BeautifulSoup(r.content,'lxml')
productlist = soup.find_all('div', class_='overlay text')
#productlist = soup.find_all(class_ = 'btnShowDetails')

for item in productlist:
  for link in item.find_all('a',class_ = 'btnShowDetails', href = True):
    productlinks.append(baseurl3 + link['href'])
print(*productlinks, sep = "\n")

"""Let's test run the script on one of these links first (i.e., any one product). And we'll loop over the rest later.

"""

testlink = 'https://chaldal.com/aci-savlon-liquid-antiseptic-1000-ml'
r = requests.get(testlink, headers = headers)
soup = BeautifulSoup(r.content, 'lxml')
try:
  nameandsubtext = soup.find('div', {'class':'nameAndSubtext'}).text.strip()
  product_name = nameandsubtext[0:-5]
  product_weight = nameandsubtext[-5:]
except AttributeError:
  pass

price = str(soup.find('span', {'itemprop':'price'}))
price = price.split('>')
product_price = price[2].split('<')[0]

description = str(soup.find('div', {'class':'details'}))
description = description.split('>')
product_description = description[2].split('<')[0]

product = {
    'name': product_name,
    'weight': product_weight,
    'price': product_price,
    'description': product_description

}

print(product)

#testlink = 'https://chaldal.com/fresh-soyabean-oil-5-ltr'
final_productlist = []
for link in productlinks:
  r = requests.get(link, headers = headers)
  soup = BeautifulSoup(r.content, 'lxml')

  try:
    nameandsubtext = soup.find('div', {'class':'nameAndSubtext'}).text.strip()
    product_name = nameandsubtext[0:-5]
    product_weight = nameandsubtext[-5:]
  except AttributeError:
    pass

  price = str(soup.find('span', {'itemprop':'price'}))
  price = price.split('>')
  product_price = price[2].split('<')[0]

  description = str(soup.find('div', {'class':'details'}))
  description = description.split('>')
  product_description = description[2].split('<')[0]

  product = {
      'name': product_name,
      'weight': product_weight,
      'price': product_price,
      'description': product_description

  }
  final_productlist.append(product)

#print(final_productlist)

"""Let's convert the data into a Pnada's data farme. In the previous step, you can also print out the list to have a quick look. There might be some items that have missing data, but this should give you an start, and we can always improve.

"""

df = pd.DataFrame(final_productlist)
print(df.head(5))

"""I've used Google Colab. An easy way to convert and store the data into csv isto mount it to your Google drive.

"""

from google.colab import drive
drive.mount('drive', force_remount=True)

df.to_csv('chaldal.csv')
!cp chaldal.csv "drive/My Drive/"